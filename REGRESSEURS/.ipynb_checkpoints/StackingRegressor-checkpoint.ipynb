{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344be584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# üîπ Chargement des donn√©es\n",
    "data = pd.read_csv(\"listings.csv\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab81e75-4727-4c9f-be60-317d468ecbfa",
   "metadata": {},
   "source": [
    "le fichier de donn√©es listings.csv est charg√© dans un DataFrame nomm√© data, et les premi√®res lignes du jeu de donn√©es sont affich√©es √† l‚Äôaide de data.head() pour visualiser rapidement la structure et le contenu des donn√©es. Ce bloc constitue donc l‚Äô√©tape initiale de pr√©paration dans un projet de r√©gression visant √† pr√©dire des prix ou valeurs continues √† partir d‚Äôun dataset type Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b84bc8-42fe-4fe1-b85c-b330a083e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîπ Nettoyage et compl√©tion\n",
    "data = data.dropna(subset=['latitude', 'longitude', 'price', 'number_of_reviews'])\n",
    "data = data[data['price'] > 0]\n",
    "data['reviews_per_month'] = data['reviews_per_month'].fillna(0)\n",
    "\n",
    "# Colonnes num√©riques : remplissage par m√©diane\n",
    "numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_cols:\n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# Colonnes cat√©gorielles : remplissage par 'unknown'\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    data[col] = data[col].fillna('unknown')\n",
    "\n",
    "# üîπ Suppression d'une colonne peu utile\n",
    "data = data.drop(columns=['neighbourhood_group'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101456b-53d2-47a6-b27d-13c373122ef8",
   "metadata": {},
   "source": [
    "Ce bloc de code r√©alise les √©tapes essentielles de nettoyage et de traitement des donn√©es manquantes, ce qui est crucial avant d'entra√Æner un mod√®le de machine learning. On commence par supprimer les lignes o√π certaines colonnes indispensables, comme la latitude, la longitude, le prix ou le nombre de commentaires (number_of_reviews), sont absentes. Ensuite, on retire les lignes o√π le prix est inf√©rieur ou √©gal √† z√©ro, car un tel prix n'a pas de sens dans le contexte d'une location Airbnb. Pour la colonne reviews_per_month, les valeurs manquantes sont remplac√©es par 0, ce qui est logique : l'absence de commentaires mensuels est interpr√©t√©e comme aucun commentaire re√ßu.\n",
    "\n",
    "Pour toutes les colonnes num√©riques restantes, les valeurs manquantes sont combl√©es par la m√©diane de chaque colonne. Cela permet d‚Äô√©viter que les valeurs extr√™mes (outliers) influencent trop le remplissage, contrairement √† une imputation par la moyenne. Du c√¥t√© des colonnes cat√©gorielles (c'est-√†-dire textuelles), toutes les valeurs manquantes sont remplac√©es par la cha√Æne de caract√®res 'unknown', afin de ne pas perdre d‚Äôinformation tout en indiquant clairement qu‚Äôil s‚Äôagit de donn√©es absentes ou non renseign√©es.\n",
    "\n",
    "Enfin, la colonne neighbourhood_group, jug√©e peu informative ou redondante dans ce cas pr√©cis, est supprim√©e du dataset. Ce nettoyage permet de garantir que le mod√®le ne sera pas perturb√© par des donn√©es manquantes ou incoh√©rentes, tout en gardant un maximum d'observations pour l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f17e5-42cc-4826-892d-e4a403761e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîπ Cr√©ation de variables suppl√©mentaires\n",
    "data['has_reviews'] = (data['number_of_reviews'] > 0).astype(int)\n",
    "data['high_availability'] = (data['availability_365'] > 180).astype(int)\n",
    "data['log_price'] = np.log1p(data['price'])\n",
    "data['reviews_density'] = data['number_of_reviews'] / (data['minimum_nights'] + 1)\n",
    "data['price_per_review'] = data['price'] / (data['number_of_reviews'] + 1)\n",
    "data['is_long_term'] = (data['minimum_nights'] > 7).astype(int)\n",
    "data['is_available_year_round'] = (data['availability_365'] >= 350).astype(int)\n",
    "data['log_reviews_density'] = np.log1p(data['reviews_density'])\n",
    "\n",
    "# üîπ Capping de certaines colonnes\n",
    "data['minimum_nights'] = data['minimum_nights'].clip(upper=30)\n",
    "data['number_of_reviews'] = data['number_of_reviews'].clip(upper=500)\n",
    "\n",
    "# üîπ S√©lection des features et cible\n",
    "features = [\n",
    "    'latitude', 'longitude', 'minimum_nights', 'number_of_reviews',\n",
    "    'reviews_per_month', 'room_type', 'calculated_host_listings_count',\n",
    "    'has_reviews', 'high_availability', 'reviews_density',\n",
    "    'price_per_review', 'is_long_term', 'is_available_year_round',\n",
    "    'log_reviews_density'\n",
    "]\n",
    "target = 'log_price'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e3986-7a40-496d-8fff-619720cead45",
   "metadata": {},
   "source": [
    "Ce bloc de code enrichit le dataset d‚ÄôAirbnb en cr√©ant de nouvelles variables d√©riv√©es, appel√©es *features d‚Äôing√©nierie*, qui ont pour but d‚Äôapporter davantage d‚Äôinformations pertinentes au mod√®le de pr√©diction. On commence par cr√©er des indicateurs binaires comme `has_reviews`, qui indique si un logement a re√ßu au moins un commentaire, ou `high_availability`, qui identifie les logements disponibles plus de 180 jours par an. Ces variables transforment des informations num√©riques en signaux facilement interpr√©tables pour un mod√®le.\n",
    "\n",
    "La variable `log_price` applique un logarithme au prix (avec `log1p` pour g√©rer les z√©ros), afin de lisser la distribution et r√©duire l‚Äôeffet des valeurs extr√™mes. On cr√©e aussi des indicateurs de densit√© d‚Äôavis comme `reviews_density`, qui divise le nombre de commentaires par le nombre de nuits minimum plus un, ou encore `price_per_review`, une estimation du prix rapport√© au nombre de commentaires re√ßus. Ces variables visent √† capturer l‚Äôactivit√© ou la popularit√© d‚Äôun logement.\n",
    "\n",
    "On ajoute aussi des indicateurs pour rep√©rer les locations longue dur√©e (`is_long_term`) et les logements disponibles presque toute l‚Äôann√©e (`is_available_year_round`). Pour certaines variables fortement dispers√©es, comme `minimum_nights` ou `number_of_reviews`, on applique un *capping*, c‚Äôest-√†-dire qu‚Äôon fixe une valeur maximale (respectivement 30 et 500), afin de limiter l‚Äôimpact des outliers sur le mod√®le.\n",
    "\n",
    "Enfin, on s√©lectionne les variables les plus pertinentes pour l‚Äôentra√Ænement du mod√®le dans la liste `features`, tandis que la variable √† pr√©dire est `log_price`, la version logarithmique du prix initial. Ces √©tapes de transformation et de s√©lection permettent de mieux structurer l'information contenue dans les donn√©es brutes et d'am√©liorer potentiellement les performances du mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06357db5-44b3-4433-9f6a-cd7d4aad6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîπ S√©paration train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# üîπ Pr√©traitement\n",
    "numeric_features = [\n",
    "    'latitude', 'longitude', 'minimum_nights', 'number_of_reviews',\n",
    "    'reviews_per_month', 'calculated_host_listings_count',\n",
    "    'reviews_density', 'price_per_review', 'log_reviews_density'\n",
    "]\n",
    "categorical_features = ['room_type', 'has_reviews', 'high_availability', 'is_long_term', 'is_available_year_round']\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1ced8-1e92-4f95-9e09-997a50d973ac",
   "metadata": {},
   "source": [
    "Ce bloc de code pr√©pare les donn√©es pour l‚Äôentra√Ænement du mod√®le en r√©alisant plusieurs √©tapes cl√©s. Tout d‚Äôabord, les donn√©es sont divis√©es en deux ensembles : un ensemble d‚Äôentra√Ænement (`X_train`, `y_train`) qui repr√©sente 75 % des donn√©es, et un ensemble de test (`X_test`, `y_test`) qui sert √† √©valuer la performance du mod√®le sur des donn√©es jamais vues, avec un d√©coupage reproductible gr√¢ce √† la graine al√©atoire fix√©e (`random_state=42`).\n",
    "\n",
    "Ensuite, on identifie les variables num√©riques et cat√©gorielles parmi les features s√©lectionn√©es pr√©c√©demment. Les variables num√©riques regroupent des mesures continues ou discr√®tes, tandis que les variables cat√©gorielles sont des indicateurs ou des cat√©gories.\n",
    "\n",
    "Pour le pr√©traitement, on cr√©e deux pipelines distincts. Le pipeline pour les variables num√©riques comprend d‚Äôabord une √©tape d‚Äôimputation qui remplace les valeurs manquantes par la m√©diane de la colonne, suivie d‚Äôune standardisation via `StandardScaler` qui met les donn√©es sur une m√™me √©chelle avec une moyenne nulle et une variance unitaire, facilitant ainsi l‚Äôapprentissage du mod√®le.\n",
    "\n",
    "Pour les variables cat√©gorielles, le pipeline remplit les valeurs manquantes avec la valeur constante 'unknown' puis encode les cat√©gories en variables binaires gr√¢ce au OneHotEncoder, permettant ainsi au mod√®le de traiter correctement les donn√©es non num√©riques.\n",
    "\n",
    "Enfin, les deux pipelines sont combin√©s dans un `ColumnTransformer` qui applique automatiquement ces transformations sp√©cifiques aux colonnes correspondantes. Ce pr√©traitement structur√© garantit que les donn√©es d‚Äôentr√©e sont nettoy√©es, mises √† l‚Äô√©chelle et encod√©es de mani√®re adapt√©e avant d‚Äô√™tre utilis√©es dans les mod√®les de machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47771f1b-f7a3-43b7-96e7-0e65081747cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üîπ D√©finition du mod√®le Stacking\n",
    "base_models = [\n",
    "    ('ridge', Ridge(alpha=1.0, random_state=42)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "    ('gbr', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "]\n",
    "\n",
    "meta_model = Ridge(alpha=0.1, random_state=42)\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('stacking', stacking)\n",
    "])\n",
    "\n",
    "# üîπ Grille √©tendue pour RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'stacking__final_estimator__alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'stacking__ridge__alpha': [0.1, 1.0, 10.0],\n",
    "    'stacking__rf__n_estimators': [100, 200],\n",
    "    'stacking__rf__max_depth': [10, 20, None],\n",
    "    'stacking__gbr__n_estimators': [100, 200],\n",
    "    'stacking__gbr__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'stacking__gbr__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=40,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# üîπ Entra√Ænement\n",
    "print(\"üîÑ Entra√Ænement du mod√®le avec RandomizedSearchCV...\")\n",
    "search.fit(X_train, y_train)\n",
    "print(\"‚úÖ Entra√Ænement termin√©.\")\n",
    "\n",
    "# üîπ R√©sultats\n",
    "print(\"\\nüîß Meilleurs param√®tres :\")\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nüìä √âvaluation du mod√®le (log-scale) :\")\n",
    "print(f\" - RMSE : {rmse:.4f}\")\n",
    "print(f\" - MAE  : {mae:.4f}\")\n",
    "print(f\" - R¬≤   : {r2:.3f}\")\n",
    "\n",
    "print(\"\\nüìä √âvaluation (prix en ‚Ç¨) :\")\n",
    "print(f\" - RMSE : {np.expm1(rmse):.2f} ‚Ç¨\")\n",
    "print(f\" - MAE  : {np.expm1(mae):.2f} ‚Ç¨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9b28a-c320-44a3-8055-730c0aa586ad",
   "metadata": {},
   "source": [
    "Ce code d√©finit et entra√Æne un mod√®le de r√©gression avanc√© bas√© sur une approche d‚Äôempilement (stacking) combinant plusieurs mod√®les de base pour am√©liorer la pr√©cision des pr√©dictions. Trois mod√®les de base sont choisis : une r√©gression Ridge, une for√™t al√©atoire (Random Forest) et un Gradient Boosting Regressor, chacun apportant une approche diff√©rente pour capturer les relations entre les variables et la cible. Ces mod√®les de base sont ensuite combin√©s par un mod√®le m√©ta, lui aussi une r√©gression Ridge, qui apprend √† optimiser la combinaison des pr√©dictions des mod√®les de base.\n",
    "\n",
    "L‚Äôensemble est int√©gr√© dans un pipeline avec le pr√©traitement des donn√©es d√©j√† d√©fini, garantissant que les donn√©es passent par les m√™mes √©tapes de nettoyage et de transformation avant d‚Äô√™tre utilis√©es pour l‚Äôapprentissage.\n",
    "\n",
    "Pour optimiser les hyperparam√®tres du mod√®le, une recherche al√©atoire (`RandomizedSearchCV`) est utilis√©e, explorant un large espace de param√®tres pour les trois mod√®les de base et le mod√®le m√©ta, comme le taux de r√©gularisation alpha pour la Ridge, le nombre d‚Äôarbres et la profondeur maximale pour la for√™t al√©atoire, ou encore le nombre d‚Äôarbres, le taux d‚Äôapprentissage et la profondeur pour le Gradient Boosting. Cette recherche est effectu√©e avec validation crois√©e pour garantir la robustesse des r√©sultats.\n",
    "\n",
    "Le mod√®le est ensuite entra√Æn√© sur l‚Äôensemble d‚Äôentra√Ænement, et les meilleurs param√®tres trouv√©s sont affich√©s. Enfin, les performances sont √©valu√©es sur l‚Äôensemble de test en calculant plusieurs m√©triques : RMSE (racine de l‚Äôerreur quadratique moyenne), MAE (erreur absolue moyenne) et R¬≤ (coefficient de d√©termination), d‚Äôabord sur les valeurs transform√©es en log, puis retranscrites en prix r√©els pour une interpr√©tation plus intuitive. Ces r√©sultats permettent d‚Äôestimer la qualit√© et la pr√©cision du mod√®le pour pr√©dire le prix des annonces Airbnb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdea566-a5e3-4784-a7da-f17b1973b468",
   "metadata": {},
   "source": [
    "## NOTES ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9224062",
   "metadata": {},
   "source": [
    "Le meilleur param√®tre trouv√© pour le m√©ta-mod√®le Ridge utilis√© dans le stacking est alpha = 0.1, ce qui indique une r√©gularisation L2 avec une p√©nalisation relativement faible, permettant un bon ajustement aux donn√©es sans surcontraindre le mod√®le. En termes d‚Äô√©valuation sur l‚Äô√©chelle logarithmique du prix, le mod√®le affiche un RMSE de 0.5114, mesurant l‚Äôerreur quadratique moyenne entre valeurs r√©elles et pr√©dites, et une MAE de 0.3716, indiquant une erreur absolue moyenne mod√©r√©e. Le coefficient de d√©termination R¬≤ est de 0.505, ce qui signifie que le mod√®le explique environ 50,5 % de la variance des prix en log, montrant une performance correcte mais perfectible. Une fois les pr√©dictions ramen√©es √† l‚Äô√©chelle r√©elle des prix en euros, le RMSE est de 0.67 ‚Ç¨ et la MAE de 0.45 ‚Ç¨, ce qui traduit l‚Äôerreur moyenne sur les prix effectifs. Le mod√®le pr√©dit les prix de quelques annonces test avec une erreur moyenne d‚Äôenviron 9,3 %, indiquant une proximit√© acceptable mais laissant place √† des am√©liorations possibles. La validation crois√©e sur 5 plis donne un RMSE moyen de 0.3083, attestant de la bonne stabilit√© et capacit√© de g√©n√©ralisation du mod√®le sans surapprentissage marqu√©. En conclusion, ce mod√®le de stacking offre une performance raisonnable avec un R¬≤ autour de 50 %, mais il reste des marges de progression possibles par l‚Äôajustement des hyperparam√®tres, l‚Äôexploration de mod√®les plus complexes ou l‚Äôenrichissement des variables utilis√©es. La validation crois√©e solide confirme la robustesse et la fiabilit√© du mod√®le pour des pr√©dictions futures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995cb1be",
   "metadata": {},
   "source": [
    "Le mod√®le pr√©sente une pr√©cision mod√©r√©e avec un R¬≤ de 0.505, ce qui signifie qu‚Äôil explique seulement 50,5 % de la variance des prix, laissant plus de 40 % des variations non captur√©es, ce qui indique une marge importante d‚Äôam√©lioration pour mieux comprendre les facteurs influen√ßant le prix. L‚Äôerreur quadratique moyenne sur les prix r√©els est de 0.67 ‚Ç¨, ce qui peut √™tre acceptable, mais montre que le mod√®le peine encore √† pr√©dire pr√©cis√©ment certains prix, ce qui peut poser probl√®me dans des contextes demandant une grande pr√©cision comme la recommandation de prix. Les caract√©ristiques utilis√©es sont relativement simples (latitude, longitude, type de chambre, nombre de nuits minimales, etc.) et peuvent ne pas refl√©ter tous les facteurs importants, par exemple les images des annonces, les √©quipements, la proximit√© de lieux populaires ou des informations sur l‚Äôh√¥te ne sont pas prises en compte. Le mod√®le actuel ne capture peut-√™tre pas suffisamment les interactions complexes entre caract√©ristiques, comme la combinaison du type de chambre avec la disponibilit√©, qui pourrait influencer davantage le prix. Il existe aussi un risque de surajustement sur certains mod√®les de base comme RandomForestRegressor, malgr√© une validation crois√©e rassurante.\n",
    "\n",
    "Pour am√©liorer le mod√®le, il serait pertinent d‚Äôajouter de nouvelles caract√©ristiques, telles que des donn√©es li√©es aux images (qualit√©, nombre d‚Äôimages), des caract√©ristiques g√©ographiques suppl√©mentaires (proximit√© de points d‚Äôint√©r√™t, attractions touristiques, centres de transport) ou des informations sur l‚Äôh√¥te (nombre d‚Äôann√©es d‚Äôactivit√©, r√©putation). Explorer des mod√®les plus avanc√©s non lin√©aires comme les r√©seaux de neurones, XGBoost, LightGBM ou CatBoost pourrait aussi aider √† mieux capturer les relations complexes. L‚Äôarchitecture du stacking pourrait √™tre optimis√©e en ajoutant d‚Äôautres mod√®les de base (SVM, KNN, r√©seaux de neurones) ou en simplifiant le stacking en r√©duisant certains mod√®les peu compl√©mentaires. Une optimisation plus pouss√©e des hyperparam√®tres, notamment via des techniques comme la Bayesian Optimization, permettrait d‚Äôam√©liorer les performances. Le feature engineering peut √©galement √™tre enrichi en cr√©ant des interactions entre caract√©ristiques existantes, par exemple en combinant latitude et longitude pour former des clusters g√©ographiques, ou en transformant certaines variables avec des encodages sp√©cifiques comme des logarithmes. La gestion des valeurs manquantes pourrait √™tre approfondie avec des mod√®les pr√©dictifs sp√©cifiques pour ces valeurs absentes.\n",
    "\n",
    "Enfin, utiliser des m√©thodes d‚Äôensemble plus avanc√©es combinant stacking, boosting ou bagging pourrait renforcer les r√©sultats. Il est aussi conseill√© de suivre la stabilit√© et la robustesse des pr√©dictions en analysant les erreurs selon la r√©gion g√©ographique ou le type de logement, afin de d√©tecter les situations o√π le mod√®le √©choue, ce qui pourrait orienter des am√©liorations cibl√©es.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
