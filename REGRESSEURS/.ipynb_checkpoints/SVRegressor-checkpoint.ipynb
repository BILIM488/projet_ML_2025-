{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae43369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 des configurations testées :\n",
      "     mean_test_score  std_test_score  param_regressor__C  \\\n",
      "66         -0.374055        0.005395                 1.0   \n",
      "60         -0.375257        0.005913                 1.0   \n",
      "72         -0.375623        0.004504                 1.0   \n",
      "129        -0.384205        0.006287               100.0   \n",
      "99         -0.384552        0.004383                10.0   \n",
      "\n",
      "     param_regressor__epsilon param_regressor__kernel param_regressor__gamma  \n",
      "66                       0.10                     rbf                  scale  \n",
      "60                       0.01                     rbf                  scale  \n",
      "72                       0.20                     rbf                  scale  \n",
      "129                      0.10                     rbf                   auto  \n",
      "99                       0.10                     rbf                   auto  \n",
      "\n",
      "MAE: 53.81\n",
      "RMSE: 115.87\n",
      "R²: 0.23\n",
      "\n",
      "Prix prédit pour la nouvelle annonce : 122.20 $ / 113.65 €\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Chargement des données\n",
    "data = pd.read_csv(\"listings.csv\")\n",
    "print(data.head())\n",
    "\n",
    "# Suppression des outliers (z-score > 3 sur 'price')\n",
    "data = data[(np.abs(zscore(data['price'])) < 3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdba5d-de52-4708-b98f-c98292f6b1fe",
   "metadata": {},
   "source": [
    "Voici un bloc explicatif unifié pour le code que tu as donné :\n",
    "\n",
    "Ce code commence par importer les bibliothèques nécessaires pour la manipulation des données, le traitement statistique et la modélisation avec scikit-learn. Il charge ensuite un jeu de données à partir d’un fichier CSV nommé \"listings.csv\" puis affiche les premières lignes pour avoir un aperçu. Pour améliorer la qualité des données, il supprime les valeurs aberrantes dans la colonne des prix en calculant le score z (z-score) de chaque valeur et en ne conservant que celles dont la valeur absolue est inférieure à 3. Cette étape permet d’éliminer les prix extrêmes qui pourraient fausser l’analyse et la modélisation ultérieure. Enfin, il affiche le nombre d’observations restantes après ce nettoyage.\n",
    "\n",
    "Veux-tu que je te fasse un autre bloc explicatif pour la suite ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2176b93-6a96-4e53-a7ba-670b2d14648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log-transformation des colonnes skewées\n",
    "skewed_cols = ['minimum_nights', 'number_of_reviews', 'reviews_per_month']\n",
    "for col in skewed_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].apply(lambda x: np.log1p(x))\n",
    "\n",
    "# Cible log-transformée\n",
    "y = np.log1p(data['price'])\n",
    "X = data.drop(columns=['price'])\n",
    "\n",
    "# Suppression des colonnes inutiles\n",
    "for col in ['name', 'last_review', 'host_name', 'neighbourhood_group']:\n",
    "    if col in X.columns:\n",
    "        X.drop(columns=col, inplace=True)\n",
    "\n",
    "# Colonnes numériques et catégorielles\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42136736-6695-461c-8259-2015806f9afc",
   "metadata": {},
   "source": [
    "Ce bloc commence par appliquer une transformation logarithmique (`log1p`) aux colonnes fortement asymétriques (`minimum_nights`, `number_of_reviews`, `reviews_per_month`) afin de réduire l’effet des valeurs extrêmes et de rendre la distribution plus normale, ce qui facilite l’apprentissage du modèle.\n",
    "\n",
    "La variable cible (`y`) correspond au prix transformé en log pour stabiliser la variance et mieux modéliser la relation. Les variables explicatives (`X`) sont obtenues en supprimant la colonne originale `price`.\n",
    "\n",
    "Ensuite, plusieurs colonnes peu informatives ou redondantes (`name`, `last_review`, `host_name`, `neighbourhood_group`) sont supprimées pour éviter de polluer le modèle avec des données non pertinentes.\n",
    "\n",
    "Enfin, le code identifie automatiquement les colonnes numériques et catégorielles dans le jeu de données, ce qui permettra d’appliquer un traitement adapté à chaque type de variable lors des étapes suivantes (prétraitement et encodage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f2c0d-3de5-4c48-a1ca-e7ebbb23a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prétraitement\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Pipeline complet\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', SelectKBest(score_func=f_regression, k='all')),\n",
    "    ('regressor', SVR())\n",
    "])\n",
    "\n",
    "# Grille d'hyperparamètres\n",
    "param_grid = {\n",
    "    'regressor__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'regressor__epsilon': [0.01, 0.1, 0.2, 0.5, 1],\n",
    "    'regressor__kernel': ['rbf', 'linear', 'poly'],\n",
    "    'regressor__gamma': ['scale', 'auto']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc179d0-366e-49bb-8686-a83ff57cd7bc",
   "metadata": {},
   "source": [
    "Ce code met en place un pipeline complet de prétraitement et de modélisation pour des données tabulaires.\n",
    "\n",
    "Pour les variables numériques, il applique d'abord une imputation par la médiane pour gérer les valeurs manquantes, puis standardise les données avec un StandardScaler afin de centrer et réduire les variables.\n",
    "\n",
    "Pour les variables catégorielles, il remplace les valeurs manquantes par la modalité la plus fréquente, puis encode ces variables en variables indicatrices (one-hot encoding) tout en ignorant les catégories inconnues lors de la prédiction.\n",
    "\n",
    "Ces transformations sont combinées avec un ColumnTransformer qui applique automatiquement le traitement adapté selon le type de variable.\n",
    "\n",
    "Ensuite, un pipeline global assemble ce prétraitement, une étape de sélection de caractéristiques avec SelectKBest (basée sur un test de régression), puis un modèle de régression par machines à vecteurs de support (SVR).\n",
    "\n",
    "Une grille d’hyperparamètres est définie pour optimiser le modèle SVR via une recherche, incluant la régularisation C, la marge d’erreur epsilon, le type de noyau (rbf, linear, poly) et le paramètre gamma contrôlant la portée de l’influence des points de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8afa48-9fd2-4647-b82f-4b75c2f686bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Séparation train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Validation croisée\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des résultats de la validation croisée\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results = cv_results.sort_values(by=\"mean_test_score\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 des configurations testées :\")\n",
    "print(cv_results[[\n",
    "    'mean_test_score', 'std_test_score',\n",
    "    'param_regressor__C',\n",
    "    'param_regressor__epsilon',\n",
    "    'param_regressor__kernel',\n",
    "    'param_regressor__gamma'\n",
    "]].head(5))\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Évaluation\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_test_exp = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test_exp, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred))\n",
    "r2 = r2_score(y_test_exp, y_pred)\n",
    "\n",
    "print(f\"\\nMAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Prédiction sur une annonce\n",
    "sample = X_test.iloc[[0]]\n",
    "predicted_log_price = best_model.predict(sample)\n",
    "predicted_price_usd = np.expm1(predicted_log_price[0])\n",
    "predicted_price_eur = predicted_price_usd * 0.93\n",
    "\n",
    "print(f\"\\nPrix prédit pour la nouvelle annonce : {predicted_price_usd:.2f} $ / {predicted_price_eur:.2f} €\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f6175-a1e3-4fce-8e72-151c0418df3f",
   "metadata": {},
   "source": [
    "Ce code définit un pipeline complet de traitement et de modélisation adapté à des données mixtes (numériques et catégorielles).\n",
    "\n",
    "La transformation des variables numériques comprend une imputation des valeurs manquantes par la médiane, suivie d’une standardisation (centrage et réduction) grâce à StandardScaler.\n",
    "\n",
    "Pour les variables catégorielles, les valeurs manquantes sont remplacées par la modalité la plus fréquente, puis elles sont converties en variables indicatrices binaires via un encodage one-hot, en ignorant les catégories inconnues lors de la phase de test.\n",
    "\n",
    "Ces deux types de transformations sont combinés via un ColumnTransformer qui applique automatiquement la bonne transformation selon le type de variable.\n",
    "\n",
    "Le pipeline complet enchaîne ce prétraitement, une étape de sélection des meilleures caractéristiques avec SelectKBest basée sur la relation linéaire avec la cible (f_regression), et un modèle de régression à support vectoriel (SVR).\n",
    "\n",
    "Enfin, une grille d’hyperparamètres est préparée pour optimiser le modèle SVR. Elle explore différentes valeurs de la régularisation C, du paramètre d’insensibilité epsilon, des types de noyau (rbf, linear, poly), et des modes de calcul de gamma (scale ou auto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e133dda-ea3e-4e87-855e-7eb05d6514e0",
   "metadata": {},
   "source": [
    "## NOTES ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e52c0",
   "metadata": {},
   "source": [
    "Le meilleur MAE moyen obtenu est de 0.374 sur les données log-transformées (base e), avec un écart type faible (\\~0.005) indiquant une bonne stabilité du modèle durant la validation croisée. Cependant, l’évaluation sur l’ensemble de test en prix réels montre une erreur moyenne absolue (MAE) d’environ 53.81 €, une RMSE de 115.87 € indiquant une erreur plus sévère sur les grandes valeurs, et un R² de seulement 0.23, ce qui signifie que le modèle n’explique que 23 % de la variance des prix réels. Cette différence traduit une perte de performance importante entre la validation croisée sur données transformées et le test final sur prix réels. Parmi les limites du modèle actuel, la transformation logarithmique bien que stabilisant la variance, rend les erreurs finales difficiles à interpréter directement. Le SVM montre une performance limitée avec de grands jeux de données hétérogènes, notamment à cause de l’encodage OneHot des variables catégorielles qui augmente la dimension, entraînant un temps de calcul élevé pour la grille SVR. Le score R² faible indique un sous-apprentissage possible, avec une incapacité à bien capturer les facteurs influents et les interactions non linéaires entre variables, surtout en présence de bruit ou peu de tuning. De plus, les données sont potentiellement déséquilibrées ou bruitées, les prix Airbnb variant beaucoup selon des critères qualitatifs (photos, description, réputation). Pour améliorer ce modèle, il serait pertinent de tester des modèles plus robustes comme Random Forest, Gradient Boosting (XGBoost, LightGBM, HistGradientBoostingRegressor) qui gèrent mieux les interactions et les données hétérogènes, d’appliquer une réduction de dimension (PCA) ou une sélection automatique de variables pour limiter la complexité due à OneHotEncoder, d’enrichir le feature engineering avec des variables comme le score d’évaluation par nuit ou le taux d’occupation, d’utiliser un encodage catégoriel avancé (Target Encoding) au lieu de OneHot pour les grosses colonnes, et d’augmenter le nombre de folds en validation croisée (5 ou 10) pour une meilleure estimation de la variance. L’exploration de modèles basés sur le deep learning pourrait aussi être envisagée si les données et ressources le permettent. En conclusion, le SVM testé, bien que stable en validation croisée, n’offre pas des performances satisfaisantes en production sur les prix Airbnb réels. L’erreur moyenne de 54 € reste élevée et le faible R² montre que le modèle ne capture pas la complexité des prix. Des modèles ensemblistes plus adaptés pourraient significativement améliorer la qualité prédictive tout en conservant une bonne robustesse face au bruit des données.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
